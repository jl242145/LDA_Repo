{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4010ee14",
   "metadata": {},
   "source": [
    "# Specificity \n",
    "Hope, O. K., Hu, D., & Lu, H. (2016). The benefits of specific risk-factor disclosures. Review of Accounting Studies, 21(4), 1005-1045."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac8dd55",
   "metadata": {},
   "source": [
    "# Code Reference\n",
    "The code is from https://www.linguisticsweb.org/doku.php?id=linguisticsweb:tutorials:automaticannotation:stanford_ner_tagger_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e81a41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a9fd584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2039426",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\Java\\jre-1.8'\n",
    "os.environ['PATH'] += os.pathsep + r'C:\\Program Files\\Java\\jre-1.8\\bin'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32b18f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to model and jar\n",
    "model = \"D:\\Data\\Stanford_NER\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\classifiers\\english.muc.7class.distsim.crf.ser.gz\"\n",
    "jar = \"D:\\Data\\Stanford_NER\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\\stanford-ner-4.2.0.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5d3c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tagger\n",
    "ner_tagger = StanfordNERTagger(model, jar, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f3cc2",
   "metadata": {},
   "source": [
    "# Test with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b391e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your input text (replace with your own file or string)\n",
    "text = \"Joe Riddle was born in Hawaii at 7:00 AM on August 4, 1961, holding 400 dollars. Also it is grate to meet you at this early hour.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f889c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Joe', 'PERSON'), ('Riddle', 'PERSON'), ('was', 'O'), ('born', 'O'), ('in', 'O'), ('Hawaii', 'LOCATION'), ('at', 'O'), ('7:00', 'TIME'), ('AM', 'TIME'), ('on', 'O'), ('August', 'DATE'), ('4', 'DATE'), (',', 'DATE'), ('1961', 'DATE'), (',', 'O'), ('holding', 'O'), ('400', 'MONEY'), ('dollars', 'MONEY'), ('.', 'O'), ('Also', 'O'), ('it', 'O'), ('is', 'O'), ('grate', 'O'), ('to', 'O'), ('meet', 'O'), ('you', 'O'), ('at', 'O'), ('this', 'O'), ('early', 'O'), ('hour', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "words = word_tokenize(text)\n",
    "# Tag\n",
    "classified_words = ner_tagger.tag(words)\n",
    "print(classified_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b241c73",
   "metadata": {},
   "source": [
    "# Practice with real text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a67ac131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    filename                                            content\n",
      "0  10098.txt  The military revenue for the fourth quarter wa...\n",
      "1  11069.txt  It's a couple of questions in there. Let me st...\n",
      "2   1157.txt  Let me start, Gregg, by saying that with respe...\n",
      "3  11681.txt  Sure, Simon. First of all, thank you for your ...\n",
      "4  11812.txt  Okay. Matt, I'll take that. And we're looking ...\n"
     ]
    }
   ],
   "source": [
    "def read_txt_files_to_df(txt_dir):\n",
    "    data = []\n",
    "    for filename in os.listdir(txt_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            filepath = os.path.join(txt_dir, filename)\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                data.append({'filename': filename, 'content': content})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Directory containing your txt files\n",
    "file_dir = r'D:\\Data\\CapitalIQ_Transcript\\Txt_TestRun_v1'\n",
    "\n",
    "# Read files into DataFrame\n",
    "df = read_txt_files_to_df(file_dir)\n",
    "print(df.head())  # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea55fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Efficient Batch NER Tagging ---\n",
    "def batch_ner_tag(text_list, tagger, batch_size=500):\n",
    "    tagged_results = []\n",
    "    total = len(text_list)\n",
    "    for i in range(0, total, batch_size):\n",
    "        batch = text_list[i:i+batch_size]\n",
    "        tokenized_batch = [word_tokenize(text) if isinstance(text, str) else [] for text in batch]\n",
    "        tagged_batch = tagger.tag_sents(tokenized_batch)\n",
    "        tagged_results.extend(tagged_batch)\n",
    "    return tagged_results\n",
    "\n",
    "# --- Count Entities ---\n",
    "def count_entities(tagged):\n",
    "    counts = Counter(tag for _, tag in tagged)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93fd1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Process DataFrame ---\n",
    "# 1. Efficient NER tagging (add ner_tags column)\n",
    "df['ner_tags'] = batch_ner_tag(df['content'].tolist(), ner_tagger, batch_size=500)\n",
    "\n",
    "# 2. Count entity types for each row (add temp counts DataFrame)\n",
    "entity_counts_df = df['ner_tags'].apply(count_entities).apply(pd.Series)\n",
    "\n",
    "# 3. Always have all 8 columns (fill missing with zeros)\n",
    "entity_columns = ['O', 'PERSON', 'LOCATION', 'ORGANIZATION', 'MONEY', 'PERCENT', 'DATE', 'TIME']\n",
    "entity_counts_df = entity_counts_df.reindex(columns=entity_columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aee0ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    filename                                            content  \\\n",
      "0  10098.txt  The military revenue for the fourth quarter wa...   \n",
      "1  11069.txt  It's a couple of questions in there. Let me st...   \n",
      "2   1157.txt  Let me start, Gregg, by saying that with respe...   \n",
      "3  11681.txt  Sure, Simon. First of all, thank you for your ...   \n",
      "4  11812.txt  Okay. Matt, I'll take that. And we're looking ...   \n",
      "\n",
      "                                            ner_tags       O  PERSON  \\\n",
      "0  [(The, O), (military, O), (revenue, O), (for, ...  1656.0     3.0   \n",
      "1  [(It, O), ('s, O), (a, O), (couple, O), (of, O...  1175.0     1.0   \n",
      "2  [(Let, O), (me, O), (start, O), (,, O), (Gregg...  6262.0    50.0   \n",
      "3  [(Sure, O), (,, O), (Simon, PERSON), (., O), (...  2313.0     1.0   \n",
      "4  [(Okay, O), (., O), (Matt, O), (,, O), (I, O),...  5066.0    19.0   \n",
      "\n",
      "   LOCATION  ORGANIZATION  MONEY  PERCENT  DATE  TIME  \n",
      "0       1.0          18.0   36.0      NaN  37.0   7.0  \n",
      "1       2.0          15.0    NaN      2.0   1.0   NaN  \n",
      "2      43.0          31.0    6.0     24.0  52.0   NaN  \n",
      "3       9.0           7.0    NaN      4.0  30.0   NaN  \n",
      "4      20.0          40.0   11.0     94.0  13.0   4.0  \n"
     ]
    }
   ],
   "source": [
    "# 4. Concatenate back to the original df\n",
    "df_final = pd.concat([df, entity_counts_df], axis=1)\n",
    "\n",
    "# --- Preview results ---\n",
    "print(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed02874b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    filename                                            content  \\\n",
      "0  10098.txt  The military revenue for the fourth quarter wa...   \n",
      "1  11069.txt  It's a couple of questions in there. Let me st...   \n",
      "2   1157.txt  Let me start, Gregg, by saying that with respe...   \n",
      "3  11681.txt  Sure, Simon. First of all, thank you for your ...   \n",
      "4  11812.txt  Okay. Matt, I'll take that. And we're looking ...   \n",
      "\n",
      "                                            ner_tags       O  PERSON  \\\n",
      "0  [(The, O), (military, O), (revenue, O), (for, ...  1656.0     3.0   \n",
      "1  [(It, O), ('s, O), (a, O), (couple, O), (of, O...  1175.0     1.0   \n",
      "2  [(Let, O), (me, O), (start, O), (,, O), (Gregg...  6262.0    50.0   \n",
      "3  [(Sure, O), (,, O), (Simon, PERSON), (., O), (...  2313.0     1.0   \n",
      "4  [(Okay, O), (., O), (Matt, O), (,, O), (I, O),...  5066.0    19.0   \n",
      "\n",
      "   LOCATION  ORGANIZATION  MONEY  PERCENT  DATE  TIME  sum_token  \n",
      "0       1.0          18.0   36.0      NaN  37.0   7.0     1758.0  \n",
      "1       2.0          15.0    NaN      2.0   1.0   NaN     1196.0  \n",
      "2      43.0          31.0    6.0     24.0  52.0   NaN     6468.0  \n",
      "3       9.0           7.0    NaN      4.0  30.0   NaN     2364.0  \n",
      "4      20.0          40.0   11.0     94.0  13.0   4.0     5267.0  \n"
     ]
    }
   ],
   "source": [
    "entity_columns = ['O', 'PERSON', 'LOCATION', 'ORGANIZATION', 'MONEY', 'PERCENT', 'DATE', 'TIME']\n",
    "df_final['sum_token'] = df_final[entity_columns].fillna(0).sum(axis=1)\n",
    "print(df_final.head())  # Shows all columns, including sum_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819772b",
   "metadata": {},
   "source": [
    "# Forward Looking Information\n",
    "Muslu, V., Radhakrishnan, S., Subramanyam, K. R., & Lim, D. (2015). Forward-looking MD&A disclosures and the information environment. Management Science, 61(5), 931-948."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bad509",
   "metadata": {},
   "source": [
    "Muslu et al classify future sentence different way I use machine learning method. Muslu et al further distinguish operations; finance; accounting and I follow the method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e8807",
   "metadata": {},
   "source": [
    "# Code reference\n",
    " https://huggingface.co/FinanceInc/finbert_fls\n",
    " This is from Allen Huang \n",
    " https://www.allenhuang.org/coding.html\n",
    "\n",
    " specific FLS means specific like date or dates or targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4c94015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.7.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.53.2-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 40.9/40.9 kB 989.3 kB/s eta 0:00:00\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from requests->transformers) (2025.7.14)\n",
      "Downloading torch-2.7.1-cp311-cp311-win_amd64.whl (216.1 MB)\n",
      "   ---------------------------------------- 0.0/216.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/216.1 MB 4.8 MB/s eta 0:00:45\n",
      "   ---------------------------------------- 1.0/216.1 MB 12.1 MB/s eta 0:00:18\n",
      "    --------------------------------------- 3.0/216.1 MB 23.7 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 6.6/216.1 MB 38.1 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 10.4/216.1 MB 54.7 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 14.3/216.1 MB 81.8 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 18.2/216.1 MB 81.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 22.0/216.1 MB 81.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 26.0/216.1 MB 81.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 30.0/216.1 MB 81.8 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 32.9/216.1 MB 73.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 36.7/216.1 MB 73.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 40.5/216.1 MB 72.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 44.3/216.1 MB 93.9 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 48.4/216.1 MB 81.8 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 52.2/216.1 MB 93.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 56.2/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 60.1/216.1 MB 93.0 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 63.8/216.1 MB 93.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 67.8/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 71.7/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 75.5/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 79.4/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 83.4/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 87.2/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 91.1/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 95.1/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 98.9/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------ -------------------- 102.8/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------- ------------------- 106.7/216.1 MB 93.0 MB/s eta 0:00:02\n",
      "   ------------------- ------------------- 110.6/216.1 MB 93.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 113.5/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 113.6/216.1 MB 59.5 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 113.8/216.1 MB 46.7 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 115.1/216.1 MB 40.9 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 119.0/216.1 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 122.9/216.1 MB 40.9 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 126.8/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ----------------------- --------------- 130.6/216.1 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 134.6/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------ -------------- 138.5/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------- ------------- 142.4/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------ 146.3/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 150.0/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------- ----------- 154.2/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ---------- 158.1/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 161.9/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ----------------------------- --------- 165.7/216.1 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 169.7/216.1 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 173.5/216.1 MB 93.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 177.4/216.1 MB 93.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 177.6/216.1 MB 65.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 177.7/216.1 MB 50.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 178.0/216.1 MB 40.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 179.9/216.1 MB 36.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 180.5/216.1 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 180.6/216.1 MB 29.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 181.0/216.1 MB 25.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 182.8/216.1 MB 24.3 MB/s eta 0:00:02\n",
      "   --------------------------------- ----- 186.6/216.1 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 190.7/216.1 MB 43.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 194.6/216.1 MB 93.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 198.4/216.1 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 201.3/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 205.2/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 209.3/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  213.2/216.1 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  214.1/216.1 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  214.3/216.1 MB 50.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  214.4/216.1 MB 40.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  214.6/216.1 MB 36.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.1/216.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.1/216.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.1/216.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.1/216.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.1/216.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  216.1/216.1 MB 34.4 MB/s eta 0:00:01\n",
      "   --------------------------------------- 216.1/216.1 MB 18.7 MB/s eta 0:00:00\n",
      "Downloading transformers-4.53.2-py3-none-any.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 3.5/10.8 MB 112.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.1/10.8 MB 91.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/10.8 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 65.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "   ---------------------------------------- 0.0/515.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 515.3/515.3 kB 31.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "   ---------------------------------------- 0.0/199.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 199.6/199.6 kB ? eta 0:00:00\n",
      "Downloading PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "   ---------------------------------------- 0.0/162.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 162.0/162.0 kB 4.7 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "   ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 308.9/308.9 kB 18.7 MB/s eta 0:00:00\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 3.4/6.3 MB 72.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 80.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 57.7 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 78.1 MB/s eta 0:00:00\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 65.3 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 32.9 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, safetensors, pyyaml, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.7.0 huggingface-hub-0.33.4 mpmath-1.3.0 networkx-3.5 pyyaml-6.0.2 safetensors-0.5.3 sympy-1.14.0 tokenizers-0.21.2 torch-2.7.1 transformers-4.53.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "960d611b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        filename                                           sentence\n",
      "0      10098.txt  The military revenue for the fourth quarter wa...\n",
      "1      10098.txt  Well, we're expecting the military business to...\n",
      "2      10098.txt  So we're looking at about $6 million in milita...\n",
      "3      10098.txt  We expect our imaging physics business to gain...\n",
      "4      10098.txt  That will most likely be in the second half of...\n",
      "...          ...                                                ...\n",
      "16239   9728.txt  Well if thatâ€™s all the questions, we like to t...\n",
      "16240   9728.txt         It was a good quarter and was a good year.\n",
      "16241   9728.txt  We made a lot of progress through the year and...\n",
      "16242   9728.txt                               Thank you very much.\n",
      "16243   9728.txt  If you haves any other questions, feel free to...\n",
      "\n",
      "[16244 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# This assumes df is your DataFrame from before\n",
    "# We'll create a new DataFrame with each sentence as a row, with its source filename\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    filename = row['filename']\n",
    "    content = row['content']\n",
    "    for sentence in sent_tokenize(content):\n",
    "        sentences.append({'filename': filename, 'sentence': sentence})\n",
    "\n",
    "sent_df = pd.DataFrame(sentences)\n",
    "\n",
    "print(sent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16e88d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence     FLS_label\n",
      "0  The military revenue for the fourth quarter wa...       Not FLS\n",
      "1  Well, we're expecting the military business to...  Specific FLS\n",
      "2  So we're looking at about $6 million in milita...       Not FLS\n",
      "3  We expect our imaging physics business to gain...  Specific FLS\n",
      "4  That will most likely be in the second half of...  Specific FLS\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "\n",
    "# Load model and tokenizer\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-fls', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-fls')\n",
    "\n",
    "# Build pipeline\n",
    "nlp = pipeline(\"text-classification\", model=finbert, tokenizer=tokenizer, truncation=True)\n",
    "\n",
    "# Function to classify a single sentence\n",
    "def classify_sentence(sentence):\n",
    "    result = nlp(str(sentence))[0]\n",
    "    return result['label']\n",
    "\n",
    "# Apply the classification to each sentence in the DataFrame\n",
    "sent_df['FLS_label'] = sent_df['sentence'].apply(classify_sentence)\n",
    "\n",
    "# If you want to see the result\n",
    "print(sent_df[['sentence', 'FLS_label']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f962392c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    filename                                           sentence     FLS_label\n",
      "0  10098.txt  The military revenue for the fourth quarter wa...       Not FLS\n",
      "1  10098.txt  Well, we're expecting the military business to...  Specific FLS\n",
      "2  10098.txt  So we're looking at about $6 million in milita...       Not FLS\n",
      "3  10098.txt  We expect our imaging physics business to gain...  Specific FLS\n",
      "4  10098.txt  That will most likely be in the second half of...  Specific FLS\n"
     ]
    }
   ],
   "source": [
    "# If you want to see the result\n",
    "print(sent_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab950b2c",
   "metadata": {},
   "source": [
    "# Add Muslu's classification for operation, finance, and accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76e8b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555d7520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f58b5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNet POS tagger\n",
    "def wordnet_pos_tags(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Preprocessing pipeline (your specified order)\n",
    "def txt_preprocess_pipeline(text):\n",
    "    # 1. Standardize text\n",
    "    standard_txt = text.lower()\n",
    "    clean_txt = re.sub(r'\\n', ' ', standard_txt)\n",
    "    clean_txt = re.sub(r'\\s+', ' ', clean_txt)\n",
    "    clean_txt = clean_txt.strip()\n",
    "\n",
    "    # 2. Tokenize\n",
    "    tokens = word_tokenize(clean_txt)\n",
    "\n",
    "    # 3. Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemma_tokens = [lemmatizer.lemmatize(token, wordnet_pos_tags(pos_tag)) for token, pos_tag in pos_tags]\n",
    "\n",
    "    # 4. Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stopwords = set([\n",
    "        'business', 'revenue', 'sale', 'market', 'million'])\n",
    "    all_stopwords = stop_words.union(custom_stopwords)\n",
    "    filtered_tokens = [w for w in lemma_tokens if w not in all_stopwords]\n",
    "\n",
    "    # 5. Remove non-alphabetic tokens\n",
    "    filtered_tokens_alpha = [\n",
    "        word for word in filtered_tokens\n",
    "        if word.isalpha() and not re.match(r'^[ivxlcdm]+$', word)\n",
    "    ]\n",
    "\n",
    "    # 6. Return tokens for n-gram processing\n",
    "    return filtered_tokens_alpha\n",
    "\n",
    "# Example file iteration function\n",
    "def iterate_txt_files(txt_dir):\n",
    "    texts = []\n",
    "    for filename in os.listdir(txt_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(txt_dir, filename), 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                txt_tokens = txt_preprocess_pipeline(text)\n",
    "                texts.append(txt_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4deed259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to dataframe\n",
    "df['tokens'] = df['content'].apply(txt_preprocess_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8901686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "NGRAM_TYPE = 'trigram'   # 'bigram', 'trigram', or 'unigram'\n",
    "\n",
    "# Build models based on the tokens column (after initial preprocessing)\n",
    "bigram = Phrases(df['tokens'], min_count=20, threshold=100)\n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram = Phrases(bigram[df['tokens']], threshold=100)\n",
    "trigram_mod = Phraser(trigram)\n",
    "\n",
    "def apply_ngrams(tokens, ngram_type):\n",
    "    if ngram_type == 'bigram':\n",
    "        return bigram_mod[tokens]\n",
    "    elif ngram_type == 'trigram':\n",
    "        return trigram_mod[bigram_mod[tokens]]\n",
    "    else:\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a373d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram transformation applied.\n"
     ]
    }
   ],
   "source": [
    "# Apply n-grams to the DataFrame\n",
    "df['tokens'] = df['tokens'].apply(lambda x: apply_ngrams(x, NGRAM_TYPE))\n",
    "\n",
    "if NGRAM_TYPE == 'bigram':\n",
    "    print(\"Bigram transformation applied.\")\n",
    "elif NGRAM_TYPE == 'trigram':\n",
    "    print(\"Trigram transformation applied.\")\n",
    "else:\n",
    "    print(\"Unigram: no n-gram transformation applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a944a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['military', 'fourth_quarter', 'versus', 'prior', 'year', 'well', 'expect', 'military', 'fairly', 'line', 'current', 'year', 'maybe', 'slightly', 'less', 'equipment', 'nonrecurring', 'look', 'military', 'fiscal', 'year', 'expect', 'imaging', 'physic', 'gain', 'traction', 'pick', 'relate', 'customer', 'meet', 'requirement', 'around', 'joint', 'commission', 'likely', 'second_half', 'fiscal', 'year', 'radiation', 'measurement', 'feel', 'international', 'go', 'pretty', 'well', 'think', 'domestic', 'well', 'believe', 'radiation', 'whole', 'negatively', 'impact', 'foreign_exchange', 'rate', 'approximately', 'base', 'estimate', 'foreign_exchange', 'rate', 'know', 'accurately', 'would', 'sit', 'rick', 'really', 'ca', 'comment', 'segment', 'level', 'data', 'apologize', 'rick', 'segment', 'level', 'data', 'able', 'put', 'press_release', 'direct', 'margin', 'dollar', 'probably', 'materially', 'correct', 'prior', 'indiscernible', 'yes', 'would', 'restate', 'income', 'tax', 'quarter', 'previously', 'file', 'fiscal', 'aggregate', 'increase', 'approximately', 'apologize', 'ca', 'specific', 'correct', 'obviously', 'tax', 'provision', 'affect', 'change', 'income', 'tax', 'normalized', 'tax', 'rate', 'whatever', 'period', 'look', 'addition', 'normalized', 'tax', 'rate', 'want', 'least', 'allow', 'shareholder', 'understand', 'dollar', 'rather', 'say', 'adjustment', 'try', 'frame', 'best', 'could', 'within', 'confines', 'disclosure', 'allow', 'make', 'time', 'well', 'always', 'policy', 'rick', 'think', 'hold', 'pretty', 'well', 'continue', 'declare', 'dividend', 'review', 'every', 'quarter', 'board', 'board', 'look', 'every', 'quarter', 'look', 'forward', 'full', 'disclosure', 'update', 'completely', 'file', 'automatic', 'extension', 'last', 'night', 'morning', 'last', 'night', 'last', 'hour', 'require', 'u', 'file', 'december', 'december', 'company', 'push', 'extremely', 'hard', 'ensure', 'happen', 'think', 'materially', 'complete', 'cautiously_optimistic', 'go', 'able', 'full', 'disclosure', 'everything', 'call', 'turn', 'case', 'push', 'hard', 'auditor', 'look', 'complete', 'audit', 'time', 'look', 'file', 'form', 'move', 'time', 'little', 'concerned', 'fact', 'holiday', 'contain', 'within', 'period', 'extension', 'work', 'hard', 'make', 'sure', 'file', 'timely', 'practical', 'say', 'mitra', 'hop', 'end', 'calendar', 'year', 'soon', 'thereafter', 'well', 'hear', 'navy', 'report', 'believe', 'last', 'quarter', 'expect', 'hear', 'response', 'navy', 'end', 'calendar', 'year', 'thereabout', 'actually', 'hear', 'reaction', 'defer', 'decision', 'spring', 'believe', 'decision', 'base', 'upon', 'funding', 'issue', 'versus', 'technology', 'issue', 'assessment', 'still', 'active', 'rfp', 'certain', 'thing', 'ask', 'assessment', 'speak', 'people', 'military', 'funding', 'matter', 'point', 'clear', 'visibility', 'go', 'make', 'decision', 'believe', 'well', 'position', 'obviously', 'nothing', 'hand', 'share', 'actually', 'relative', 'verifii', 'project', 'believe', 'refer', 'operational', 'cost', 'take', 'project', 'get', 'deploy', 'fiscal', 'invest', 'r', 'project', 'expect', 'invest', 'additional', 'r', 'fiscal', 'year', 'another', 'capital', 'expenditure', 'technology', 'make', 'significant', 'project', 'process', 'sorry', 'make', 'significant', 'progress', 'r', 'soft', 'launch', 'recent', 'rsna', 'industry', 'convention', 'chicago', 'month', 'december', 'point', 'look', 'commercially', 'launch', 'sometime', 'calendar', 'year', 'continue', 'research', 'test', 'go', 'commercialization', 'process', 'achieving', 'operational', 'saving', 'would', 'still', 'put', 'range', 'previously', 'disclose', 'believe', 'begin', 'occur', 'probably', 'second_half', 'want', 'caution', 'shareholder', 'ramp', 'type', 'deployment', 'say', 'want', 'absolutely', 'certain', 'customer', 'sense', 'reliable', 'trustworthy', 'measure', 'radiation', 'osl', 'technology', 'probably', 'deploy', 'slow', 'first', 'ramp', 'okay', 'benefit', 'go', 'probably', 'materially', 'fiscal', 'time', 'realize', 'help', 'well', 'right', 'digest', 'major', 'contract', 'win', 'second_half', 'fiscal', 'year', 'image', 'physic', 'grow', 'fairly', 'rapidly', 'small', 'base', 'see', 'synergy', 'create', 'dosimetry', 'open', 'new', 'opportunity', 'u', 'large', 'growth', 'medical', 'physic', 'go', 'come', 'imaging', 'physic', 'area', 'therapy', 'physic', 'area', 'get', 'less', 'traction', 'around', 'premier', 'contract', 'imaging', 'physic', 'get', 'substantial', 'traction', 'relate', 'contract', 'fiscal', 'year', 'estimate', 'estimate', 'increase', 'imaging', 'prior', 'year', 'well', 'cautiously_optimistic', 'new', 'product', 'obviously', 'late', 'stage', 'fda', 'approval', 'process', 'could', 'meaningful', 'contribution', 'time', 'think', 'successful', 'would', 'probably', 'seek', 'oem', 'type', 'relationship', 'conceivably', 'even', 'bip', 'party', 'take', 'royalty', 'still', 'play', 'would', 'say', 'correct', 'kind', 'core', 'acquire', 'hop', 'bad', 'improved', 'result', 'several', 'month', 'much', 'good', 'trajectory', 'experience', 'previously', 'kind', 'remain', 'cautiously_optimistic', 'yes', 'would', 'say', 'look', 'whole', 'would', 'say', 'demand', 'relatively_flat', 'number', 'people', 'monitor', 'relatively_flat', 'certain', 'organization', 'make', 'decision', 'monitor', 'people', 'view', 'less', 'risk', 'past', 'may', 'monitor', 'large', 'group', 'people', 'radiation', 'measurement', 'see', 'convergence', 'image', 'physic', 'radiation', 'monitoring', 'customer', 'perspective', 'also', 'see', 'opportunity', 'provide', 'service', 'customer', 'able', 'provide', 'image', 'physic', 'kind', 'elevated', 'u', 'talk', 'customer', 'another', 'significant', 'driver', 'joint', 'commission', 'believe', 'drive', 'imaging', 'physic', 'also', 'driving', 'information', 'radiation', 'management', 'call', 'enterprise', 'radiation', 'solution', 'customer', 'seek', 'organize', 'provide', 'kind', 'solution', 'customer', 'go', 'beyond', 'badge', 'person', 'measuring', 'radiation', 'provide', 'solution', 'service', 'around', 'radiation', 'customer', 'wo', 'lie', 'regulate', 'definite', 'plus', 'create', 'demand', 'see', 'benefit', 'dosimetry', 'come', 'joint', 'commission', 'well', 'rise', 'speak', 'organization', 'allow', 'u', 'bring', 'service', 'table', 'whether', 'informatics', 'consult', 'service', 'image', 'train', 'even', 'see', 'opportunity', 'training', 'come', 'table', 'think', 'idea', 'premier', 'radiation', 'solution', 'company', 'something', 'go', 'continue', 'push', 'think', 'gain', 'traction', 'create', 'new', 'opportunity', 'lady', 'gentleman', 'conclude', 'call', 'today', 'thank_join', 'u', 'wish', 'great', 'holiday_season']]\n",
      "Doc 1 n-grams: ['fourth_quarter', 'second_half', 'foreign_exchange', 'foreign_exchange', 'press_release', 'cautiously_optimistic', 'second_half', 'second_half', 'cautiously_optimistic', 'cautiously_optimistic', 'relatively_flat', 'relatively_flat', 'thank_join', 'holiday_season']\n",
      "Doc 2 n-grams: ['little_bit', 'little_bit']\n",
      "Doc 3 n-grams: ['lung_cancer', 'remain_committed', 'fourth_quarter', 'cell_lung_cancer', 'lung_cancer', 'second_half', 'unmet_need', 'cell_lung_cancer', 'lung_cancer', 'remain_committed', 'lung_cancer', 'cell_lung_cancer', 'lung_cancer', 'lung_cancer', 'phase_study', 'lung_cancer', 'fourth_quarter', 'technical_difficulty', 'conference_call', 'across_board', 'lung_cancer', 'remain_committed', 'cell_lung_cancer', 'lung_cancer', 'lung_cancer', 'foreign_exchange', 'cell_lung_cancer', 'relatively_flat', 'fourth_quarter', 'fourth_quarter', 'fourth_quarter', 'technical_difficulty', 'lung_cancer', 'cell_lung_cancer', 'lung_cancer', 'phase_study', 'lung_cancer', 'lung_cancer', 'fourth_quarter', 'across_board', 'gross_margin', 'gross_margin', 'gross_margin', 'tax_reform', 'supply_chain', 'little_bit', 'operating_expense', 'operating_expense', 'operating_expense', 'remain_committed', 'lung_cancer']\n",
      "Doc 4 n-grams: ['little_bit', 'cable_tv', 'cable_tv', 'north_american', 'cable_tv', 'prepared_remark', 'fourth_quarter', 'second_half', 'prepared_remark', 'prepared_remark', 'second_half', 'second_half', 'prepared_remark', 'cable_tv', 'cable_tv', 'cable_tv', 'keep_mind', 'cable_tv', 'cable_tv', 'cable_tv', 'cable_tv', 'prepared_remark', 'second_half', 'supply_chain', 'supply_chain', 'supply_chain', 'supply_chain', 'second_half', 'thank_join']\n",
      "Doc 5 n-grams: ['cash_flow', 'cash_flow', 'prepared_remark', 'cash_flow', 'cash_flow', 'little_bit', 'little_bit', 'relatively_flat', 'relatively_flat', 'relatively_flat', 'little_bit', 'little_bit', 'constant_currency', 'little_bit', 'little_bit', 'cash_flow', 'professional_service', 'gross_margin', 'top_line', 'gross_margin', 'gross_margin', 'keep_mind', 'new_york', 'little_bit', 'new_york']\n"
     ]
    }
   ],
   "source": [
    "texts = df['tokens'].tolist()\n",
    "print(texts[:1])\n",
    "for idx, doc in enumerate(df['tokens'].tolist()[:5]):\n",
    "    ngrams = [token for token in doc if '_' in token]\n",
    "    print(f\"Doc {idx+1} n-grams:\", ngrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "afa3b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation_keywords_ngram = [\n",
    "    \"performance\", \"perform\", \"sales\", \"revenue\", \"earnings\", \"income\", \"profit\", \"loss\", \"expense\", \"ebt\", \"ebit\", \"ebitda\",\n",
    "    \"depreciation\", \"amortization\", \"administrative\", \"cost_of_sales\", \"cost_of_goods_sold\", \"cogs\", \"tax\",\n",
    "    \"impairment\", \"margin\", \"working_capital\", \"receivable\", \"payable\", \"inventory\", \"materials\", \"supplies\", \"bad_debt\",\n",
    "    \"doubtful_account\", \"allowance\", \"collect\", \"accrual\", \"operating_cash_flow\", \"cash_flow_from_operations\", \"cash_flow_from_operating\",\n",
    "    \"free_cash_flow\", \"bankruptcy\", \"chapter_7\", \"chapter_11\", \"operations\", \"operating\", \"operational\", \"product\", \"service\", \"technology\",\n",
    "    \"contract\", \"overhead\", \"vendor\", \"supplier\", \"consumer\", \"customer\", \"client\", \"marketing\", \"order\", \"backlog\", \"advertising\",\n",
    "    \"commission\", \"import\", \"export\", \"freight\", \"transportation\", \"utilities\", \"energy\", \"unit\", \"power\", \"compete\", \"competitive\", \"demand\",\n",
    "    \"supply\", \"market\", \"business\", \"segment\", \"subsidy\", \"industry\", \"outsource\", \"promotion\",  \"compensation\", \"salary\",\n",
    "    \"bonus\", \"grant\", \"award\", \"pension\", \"retirement\", \"health_care\", \"employee\", \"labor\", \"union\", \"director\", \"chairman\", \"president\",\n",
    "    \"ceo\", \"cfo\", \"coo\", \"cio\", \"manager\", \"executive\", \"worker\", \"economic\", \"world\", \"country\", \"population\", \"environment\", \"government\", \"inflation\",\"write-off\",\n",
    "]\n",
    "\n",
    "investment_keywords_ngram = [\n",
    "    \"research\", \"develop\", \"r&d\", \"project\", \"invest\", \"expand\", \"dispose\", \"asset_sale\", \"asset_purchase\", \"spend\", \"capital_expenditure\", \"capex\", \"acquire\", \n",
    "    \"construct\", \"install\", \"capacity\", \"relocate\", \"remodel\", \"refresh\", \"overhaul\", \"upgrade\", \"maintain\", \"repair\", \"open\",\n",
    "    \"close\",  \"pp&e\", \"subsidiary\", \"joint_venture\", \"jv\", \"partner\", \"license\", \"patent\", \"goodwill\", \n",
    "]\n",
    "\n",
    "finance_keywords_ngram = [\n",
    "    \"finance\", \"financing\", \"financial\", \"liquid\", \"borrow\", \"covenant\", \"debt\", \"debenture\", \"principal\", \"creditor\", \"liability\",\n",
    "    \"equity\", \"capital_resource\", \"loan\", \"line_of_credit\", \"leverage\", \"fund\", \"repurchase\", \"stock_purchase\", \"share_purchase\",\n",
    "    \"commercial_paper\", \"bank_credit\", \"pay_interest\", \"principal\", \"swap\", \"lease\", \"hedge\", \"dividend\", \"interest\"\n",
    "]\n",
    "\n",
    "accounting_keywords_ngram = [\n",
    "    \"accounting\", \"gaap\", \"fas\", \"sfas\", \"fasb\", \"sec\", \"contingency\", \"record\", \"impairment_test\", \"financial_statement\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "684ffe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    filename                                           sentence     FLS_label  \\\n",
      "0  10098.txt  The military revenue for the fourth quarter wa...       Not FLS   \n",
      "1  10098.txt  Well, we're expecting the military business to...  Specific FLS   \n",
      "2  10098.txt  So we're looking at about $6 million in milita...       Not FLS   \n",
      "3  10098.txt  We expect our imaging physics business to gain...  Specific FLS   \n",
      "4  10098.txt  That will most likely be in the second half of...  Specific FLS   \n",
      "\n",
      "                                              lemmas  operation  finance  \\\n",
      "0  [military, revenue, fourth, quarter, million, ...          1        0   \n",
      "1  [well, expect, military, business, fairly, lin...          1        0   \n",
      "2            [look, million, military, fiscal, year]          0        0   \n",
      "3  [expect, image, physic, business, gain, tracti...          1        0   \n",
      "4               [likely, second, half, fiscal, year]          0        0   \n",
      "\n",
      "   accounting                                      lemmas_ngrams  investment  \n",
      "0           0  [military, revenue, fourth, quarter, million, ...           0  \n",
      "1           0  [well, expect, military, business, fairly, lin...           0  \n",
      "2           0            [look, million, military, fiscal, year]           0  \n",
      "3           0  [expect, image_physic, business, gain, tractio...           0  \n",
      "4           0               [likely, second, half, fiscal, year]           0  \n"
     ]
    }
   ],
   "source": [
    "def contains_keyword(lemmas, keywords):\n",
    "    return int(any(word in keywords for word in lemmas))\n",
    "\n",
    "sent_df['operation'] = sent_df['lemmas'].apply(lambda x: contains_keyword(x, operation_keywords))\n",
    "sent_df['investment'] = sent_df['lemmas'].apply(lambda x: contains_keyword(x, investment_keywords))\n",
    "sent_df['finance'] = sent_df['lemmas'].apply(lambda x: contains_keyword(x, finance_keywords))\n",
    "sent_df['accounting'] = sent_df['lemmas'].apply(lambda x: contains_keyword(x, accounting_keywords))\n",
    "\n",
    "print(sent_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74bf938",
   "metadata": {},
   "source": [
    "# Sentiment Analysis \n",
    "Code reference\n",
    "https://huggingface.co/yiyanghkust/finbert-tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "44bbbde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sentence finbert_sentiment  \\\n",
      "0  The military revenue for the fourth quarter wa...           Neutral   \n",
      "1  Well, we're expecting the military business to...           Neutral   \n",
      "2  So we're looking at about $6 million in milita...           Neutral   \n",
      "3  We expect our imaging physics business to gain...          Positive   \n",
      "4  That will most likely be in the second half of...           Neutral   \n",
      "\n",
      "   finbert_sentiment_score  \n",
      "0                 0.998738  \n",
      "1                 0.999335  \n",
      "2                 0.999997  \n",
      "3                 1.000000  \n",
      "4                 0.999997  \n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "# Create the pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)\n",
    "\n",
    "# Analyze all sentences in your DataFrame (batched for speed)\n",
    "sentences = sent_df['sentence'].tolist()\n",
    "results = nlp(sentences)\n",
    "\n",
    "# Convert Hugging Face's output to DataFrame columns\n",
    "# If you want only the label, or both label and score, you can extract as below\n",
    "sent_df['finbert_sentiment'] = [res['label'] for res in results]\n",
    "sent_df['finbert_sentiment_score'] = [res['score'] for res in results]\n",
    "\n",
    "\n",
    "print(sent_df[['sentence', 'finbert_sentiment', 'finbert_sentiment_score']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a396c",
   "metadata": {},
   "source": [
    "# Fog index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "83b4b894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting py-readability-metrics\n",
      "  Downloading py_readability_metrics-1.4.5-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from py-readability-metrics) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from nltk->py-readability-metrics) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from nltk->py-readability-metrics) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from nltk->py-readability-metrics) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from nltk->py-readability-metrics) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\michael\\github\\github_lda\\lda_venv\\lib\\site-packages (from click->nltk->py-readability-metrics) (0.4.6)\n",
      "Downloading py_readability_metrics-1.4.5-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: py-readability-metrics\n",
      "Successfully installed py-readability-metrics-1.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install py-readability-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6db3661a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readability import Readability\n",
    "\n",
    "def get_fog(text):\n",
    "    # skip calculation for short texts\n",
    "    if not isinstance(text, str) or len(text.split()) < 100:\n",
    "        return None\n",
    "    try:\n",
    "        r = Readability(text)\n",
    "        gf = r.gunning_fog()\n",
    "        return gf.score\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "# Example: assuming 'content' is the column with your text\n",
    "df['fog_index'] = df['content'].apply(get_fog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eef55d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     filename                                            content  fog_index\n",
      "0   10098.txt  The military revenue for the fourth quarter wa...  13.541756\n",
      "1   11069.txt  It's a couple of questions in there. Let me st...  12.231049\n",
      "2    1157.txt  Let me start, Gregg, by saying that with respe...  12.742846\n",
      "3   11681.txt  Sure, Simon. First of all, thank you for your ...  13.452771\n",
      "4   11812.txt  Okay. Matt, I'll take that. And we're looking ...  11.566013\n",
      "..        ...                                                ...        ...\n",
      "95   8022.txt  Kathryn, we got some favorable mix, but also a...  13.701432\n",
      "96   8525.txt  Right. Exactly. I think we've got to make a de...  10.156648\n",
      "97   8982.txt  Yes. So just to -- going back to 2021, overall...   9.915541\n",
      "98   9240.txt  Joe, that's absolutely correct. You analyzed i...  11.893798\n",
      "99   9728.txt  I think itâ€™s a blend of a couple of things. I ...  13.326679\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
